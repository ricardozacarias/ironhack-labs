{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import tweepy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the url\n",
    "url = 'https://github.com/trending/developers'\n",
    "\n",
    "# get the response and cook the soup\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the class for the name of each developer\n",
    "![title](name_tag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (Á•ûÊ•ΩÂùÇË¶ö„ÄÖ)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kentaro Wada (wkentaro)',\n",
       " 'Eugen Rochko (Gargron)',\n",
       " 'Armin Ronacher (mitsuhiko)',\n",
       " 'Arvid Norberg (arvidn)',\n",
       " 'David Fowler (davidfowl)',\n",
       " 'Tim Pope (tpope)',\n",
       " 'Samuel Reed (STRML)',\n",
       " 'Paulus Schoutsen (balloob)',\n",
       " 'Navdeep Gill (navdeep-G)',\n",
       " 'Ahmad Alfy (ahmadalfy)',\n",
       " 'Maarten Breddels (maartenbreddels)',\n",
       " 'Aymeric Augustin (aaugustin)',\n",
       " 'Philippe R√©my (philipperemy)',\n",
       " 'Alexandru Nedelcu (alexandru)',\n",
       " 'Tim Meusel (bastelfreak)',\n",
       " 'Gleb Bahmutov (bahmutov)',\n",
       " 'Kevin Titor (egoist)',\n",
       " 'Frank S. Thomas (fthomas)',\n",
       " 'Huan (ÊùéÂçìÊ°ì) (huan)',\n",
       " 'Adam Geitgey (ageitgey)',\n",
       " 'Lukas Taegert-Atkinson (lukastaegert)',\n",
       " 'Stefano Gottardo (CastagnaIT)',\n",
       " 'Martin Krasser (krasserm)',\n",
       " 'ÊñáÁøº (wenzhixin)',\n",
       " 'Koen Kanters (Koenkk)']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all people under h1 tag and additional class\n",
    "people_tags = soup.find_all('h1', {\"class\": \"h3 lh-condensed\"})\n",
    "\n",
    "# get the text of the child of each element in list\n",
    "people = [tag.a.text.strip() for tag in people_tags]\n",
    "\n",
    "people_nick_tags = soup.find_all('p', {\"class\": \"f4\"})\n",
    "people_nick = [tag.text.strip() for tag in people_nick_tags][1:]\n",
    "\n",
    "hot_devs = [people[i] + ' (' + people_nick[i] + ')' for i in range(len(people))]\n",
    "hot_devs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "\n",
    "# get the response and cook the soup\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('domlysz', 'BlenderGIS'),\n",
       " ('facebookresearch', 'pytorch3d'),\n",
       " ('facebookresearch', 'meshrcnn'),\n",
       " ('public-apis', 'public-apis'),\n",
       " ('ReFirmLabs', 'binwalk'),\n",
       " ('TheAlgorithms', 'Python'),\n",
       " ('ageitgey', 'face_recognition'),\n",
       " ('tensorflow', 'models'),\n",
       " ('Manisso', 'fsociety'),\n",
       " ('nvbn', 'thefuck'),\n",
       " ('cycz', 'jdBuyMask'),\n",
       " ('cyberman219', 'cloudflare-vpn-for-desktop'),\n",
       " ('python', 'cpython'),\n",
       " ('ytdl-org', 'youtube-dl'),\n",
       " ('KevinMusgrave', 'pytorch-metric-learning'),\n",
       " ('sherlock-project', 'sherlock'),\n",
       " ('aivivn', 'd2l-vn'),\n",
       " ('deepfakes', 'faceswap'),\n",
       " ('complexdb', 'zincbase'),\n",
       " ('hzy46', 'Deep-Learning-21-Examples'),\n",
       " ('EvilCult', 'iptv-m3u-maker'),\n",
       " ('programthink', 'zhao'),\n",
       " ('3b1b', 'manim'),\n",
       " ('airbus-cert', 'comida'),\n",
       " ('pytorch', 'examples')]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all repositories under h1 tag and additional class\n",
    "repos_tags = soup.find_all('h1', {\"class\": \"h3 lh-condensed\"})\n",
    "\n",
    "# get the text of the child of each element in list\n",
    "repos_users = [tag.span.text.strip()[:-2] for tag in repos_tags]\n",
    "repos_names = [tag.a.text.replace('\\n', '').split('/')[-1].strip() for tag in repos_tags]\n",
    "\n",
    "repos = list(zip(repos_users, repos_names))\n",
    "repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/1/1b/Semi-protection-shackle.svg/20px-Semi-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/44/The_Walt_Disney_Company_Logo.svg/120px-The_Walt_Disney_Company_Logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/wikimedia-button.png',\n",
       " '/static/images/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all repositories under img tag\n",
    "find_main_page = soup.find_all('img')\n",
    "\n",
    "# get the link\n",
    "images = [tag.get('src') for tag in find_main_page]\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on actually displaying the images in the notebook\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "x = []\n",
    "for img in images:\n",
    "    x += [Image(url= img)]\n",
    "\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 1 - General Provisions',\n",
       " 'Title 2 - The Congress',\n",
       " 'Title 3 - The President',\n",
       " 'Title 4 - Flag and Seal, Seat of Government, and the States',\n",
       " 'Title 5 - Government Organization and Employees',\n",
       " 'Title 6 - Domestic Security',\n",
       " 'Title 7 - Agriculture',\n",
       " 'Title 8 - Aliens and Nationality',\n",
       " 'Title 9 - Arbitration',\n",
       " 'Title 10 - Armed Forces',\n",
       " 'Title 11 - Bankruptcy',\n",
       " 'Title 12 - Banks and Banking',\n",
       " 'Title 13 - Census',\n",
       " 'Title 14 - Coast Guard',\n",
       " 'Title 15 - Commerce and Trade',\n",
       " 'Title 16 - Conservation',\n",
       " 'Title 17 - Copyrights',\n",
       " 'Title 19 - Customs Duties',\n",
       " 'Title 21 - Food and Drugs',\n",
       " 'Title 22 - Foreign Relations and Intercourse',\n",
       " 'Title 23 - Highways',\n",
       " 'Title 24 - Hospitals and Asylums',\n",
       " 'Title 25 - Indians',\n",
       " 'Title 27 - Intoxicating Liquors',\n",
       " 'Title 28 - Judiciary and Judicial Procedure',\n",
       " 'Title 29 - Labor',\n",
       " 'Title 30 - Mineral Lands and Mining',\n",
       " 'Title 31 - Money and Finance',\n",
       " 'Title 32 - National Guard',\n",
       " 'Title 33 - Navigation and Navigable Waters',\n",
       " 'Title 34 - Crime Control and Law Enforcement',\n",
       " 'Title 35 - Patents',\n",
       " 'Title 36 - Patriotic and National Observances, Ceremonies, and Organizations',\n",
       " 'Title 37 - Pay and Allowances of the Uniformed Services',\n",
       " \"Title 38 - Veterans' Benefits\",\n",
       " 'Title 39 - Postal Service',\n",
       " 'Title 40 - Public Buildings, Property, and Works',\n",
       " 'Title 41 - Public Contracts',\n",
       " 'Title 42 - The Public Health and Welfare',\n",
       " 'Title 43 - Public Lands',\n",
       " 'Title 44 - Public Printing and Documents',\n",
       " 'Title 45 - Railroads',\n",
       " 'Title 46 - Shipping',\n",
       " 'Title 47 - Telecommunications',\n",
       " 'Title 48 - Territories and Insular Possessions',\n",
       " 'Title 49 - Transportation',\n",
       " 'Title 50 - War and National Defense',\n",
       " 'Title 51 - National and Commercial Space Programs',\n",
       " 'Title 52 - Voting and Elections',\n",
       " 'Title 53 [Reserved]',\n",
       " 'Title 54 - National Park Service and Related Programs']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all repositories under img tag\n",
    "find_main_page = soup.find_all('div', {'class':'usctitle'})\n",
    "\n",
    "titles = [tag.text.replace('Ÿ≠', '').strip() for tag in find_main_page if \"Title\" in tag.text]\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yaser Abdel Said',\n",
       " 'Jason Derek Brown',\n",
       " 'Alexis Flores',\n",
       " 'Eugene Palmer',\n",
       " 'Santiago Villalba Mederos',\n",
       " 'Rafael Caro-quintero',\n",
       " 'Robert William Fisher',\n",
       " 'Bhadreshkumar Chetanbhai Patel',\n",
       " 'Arnoldo Jimenez',\n",
       " 'Alejandro Rosales Castillo']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "wanted_tags = soup.find_all('h3', {'class': 'title'})\n",
    "\n",
    "most_wanted = [' '.join([x.capitalize() for x in tag.text.strip().lower().split()]) for tag in wanted_tags]\n",
    "most_wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i ran and saved my credentials in memory (to test it you should use your own credentials)\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter user handle:SouthPark\n",
      "User: @SouthPark\n",
      "Number of tweets: 47071\n"
     ]
    }
   ],
   "source": [
    "user = input('Enter user handle:')\n",
    "\n",
    "get_user = api.get_user(user)\n",
    "print('User: @' + get_user.screen_name)\n",
    "print('Number of tweets: ' + str(get_user.statuses_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter user handle:SouthPark\n",
      "User: @SouthPark\n",
      "Followers: 3004550\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "user = input('Enter user handle:')\n",
    "\n",
    "get_user = api.get_user(user)\n",
    "print('User: @' + get_user.screen_name)\n",
    "print('Followers: ' + str(get_user.followers_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('English', '5.994.000+ articles'),\n",
       " ('Êó•Êú¨Ë™û', '1.185.000+ Ë®ò‰∫ã'),\n",
       " ('Deutsch', '2.385.000+ Artikel'),\n",
       " ('Espa√±ol', '1.571.000+ art√≠culos'),\n",
       " ('–†—É—Å—Å–∫–∏–π', '1.590.000+ —Å—Ç–∞—Ç–µ–π'),\n",
       " ('Fran√ßais', '2.171.000+ articles'),\n",
       " ('Italiano', '1.576.000+ voci'),\n",
       " ('‰∏≠Êñá', '1.090.000+ Ê¢ùÁõÆ'),\n",
       " ('Portugu√™s', '1.018.000+ artigos'),\n",
       " ('Polski', '1.379.000+ hase≈Ç')]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "languages_tags = soup.find_all('div', {'class': 'central-featured-lang'})\n",
    "\n",
    "languages = [tag.strong.text for tag in languages_tags]\n",
    "articles = [tag.small.text.replace('\\xa0', '.') for tag in languages_tags]\n",
    "\n",
    "lang_art = list(zip(languages, articles))\n",
    "lang_art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tags = soup.find_all('h2')\n",
    "\n",
    "datasets = [tag.a.text for tag in dataset_tags]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "      <th>% of the World population(March 2019)</th>\n",
       "      <th>Language familyBranch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "      <td>11.922</td>\n",
       "      <td>Sino-TibetanSinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "      <td>5.994</td>\n",
       "      <td>Indo-EuropeanRomance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "      <td>4.922</td>\n",
       "      <td>Indo-EuropeanGermanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Hindi (Sanskritised Hindustani)</td>\n",
       "      <td>341</td>\n",
       "      <td>4.429</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>228</td>\n",
       "      <td>2.961</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>Czech</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0.139</td>\n",
       "      <td>Indo-EuropeanBalto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>88</td>\n",
       "      <td>Ta Ωizzi-Adeni Arabic</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.136</td>\n",
       "      <td>AfroasiaticSemitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>Uyghur</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.135</td>\n",
       "      <td>TurkicKarluk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>Min Dong Chinese</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>Sino-TibetanSinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>Sylheti</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                         Language Speakers(millions)  \\\n",
       "0     1                 Mandarin Chinese                918   \n",
       "1     2                          Spanish                480   \n",
       "2     3                          English                379   \n",
       "3     4  Hindi (Sanskritised Hindustani)                341   \n",
       "4     5                          Bengali                228   \n",
       "..  ...                              ...                ...   \n",
       "86   87                            Czech               10.7   \n",
       "87   88             Ta Ωizzi-Adeni Arabic               10.5   \n",
       "88   89                           Uyghur               10.4   \n",
       "89   90                 Min Dong Chinese               10.3   \n",
       "90   91                          Sylheti               10.3   \n",
       "\n",
       "   % of the World population(March 2019)      Language familyBranch  \n",
       "0                                 11.922        Sino-TibetanSinitic  \n",
       "1                                  5.994       Indo-EuropeanRomance  \n",
       "2                                  4.922      Indo-EuropeanGermanic  \n",
       "3                                  4.429    Indo-EuropeanIndo-Aryan  \n",
       "4                                  2.961    Indo-EuropeanIndo-Aryan  \n",
       "..                                   ...                        ...  \n",
       "86                                 0.139  Indo-EuropeanBalto-Slavic  \n",
       "87                                 0.136         AfroasiaticSemitic  \n",
       "88                                 0.135               TurkicKarluk  \n",
       "89                                 0.134        Sino-TibetanSinitic  \n",
       "90                                 0.134    Indo-EuropeanIndo-Aryan  \n",
       "\n",
       "[91 rows x 5 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find table by class\n",
    "table = soup.find_all('table', attrs = {'class':'wikitable'})[0]\n",
    "\n",
    "# get rows by td\n",
    "rows = table.find_all('td')\n",
    "\n",
    "# get columns by th\n",
    "columns = table.find_all('th')\n",
    "\n",
    "# get language name and remove references\n",
    "language_table = [tag.text.replace('\\n', '')[:-3] if '[' in tag.text.replace('\\n', '') else tag.text.replace('\\n', '') for tag in rows]\n",
    "\n",
    "# remove references\n",
    "columns = [tag.text.replace('\\n', '')[:-3] if '[' in tag.text.replace('\\n', '') else tag.text.replace('\\n', '') for tag in columns]\n",
    "\n",
    "# make dictionary\n",
    "languages_dict = {columns[i]:language_table[i::len(columns)] for i in range(len(columns))}\n",
    "\n",
    "# make df\n",
    "languages_df = pd.DataFrame(languages_dict)\n",
    "languages_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter user handle:SouthPark\n",
      "How many tweets? (max 900)100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['üó≥',\n",
       " '@CheebaTroupe Shablagoo!',\n",
       " '@humanshapedfish üî•',\n",
       " 'Satan wants you to go to your local library üìñ https://t.co/mbkYrGtF2E',\n",
       " 'Taco taco! Burrito burrito! üíã https://t.co/I3QSEuHCrG',\n",
       " 'Here we go! https://t.co/QrXHaldp4v',\n",
       " \"It's game day! Who are you rooting for to win? #SuperBowl https://t.co/yVLdZCjsAy\",\n",
       " 'RT @tegridyfarms: Tegridy Breakfast: breakfast of champions üèÜ https://t.co/FNERRKfmJW',\n",
       " 'Wow. You guys really love animals. https://t.co/mq2IPePr8I',\n",
       " 'Cannot believe it. \\n\\nWatch \"Season Finale\" - https://t.co/BF8B8odZuC https://t.co/TdizEKpwwO',\n",
       " 'RT @sbaughard: @SouthPark  when your son turns 13! @WGNMorningNews @TheEllenShow https://t.co/HBaTEGmcah',\n",
       " '@McKenzie_Nick Spaaaace casshhh https://t.co/6wAEYsUrGE',\n",
       " 'Happy #NationalPieDay to Scott Malkinson and Scott Malkinson only ü•ß https://t.co/zOexo4SgfC',\n",
       " \"I'm a saaaad panda üêº https://t.co/8Mu9GnafCx\",\n",
       " 'Oh my God, they killed Kenny!',\n",
       " \"If you french fry when you should pizza, you're gonna have a bad time.\",\n",
       " 'Phase 1: Collect underpants\\nPhase 2: ‚ùì\\nPhase 3: Profit https://t.co/ZQIH5RcI64',\n",
       " 'I need about tree fiddy',\n",
       " 'Woo-hoo! You made it through the first full week of the year üôå#FridayFeeling https://t.co/bsnSAR9IP4',\n",
       " '@theblackoutclub üî¶ https://t.co/AMw3Y0ZOua',\n",
       " \"I'm not your guy, friend! https://t.co/hPrLBwzJVO\",\n",
       " \"Don't forget to bring a towel! @tegridyfarms https://t.co/pGJ8ZIalHZ\",\n",
       " \"Led by the power of his microbiome, Kyle finds enough of Tom Brady's poop to save the town, and the universe. Watch ‚ÄúTurd Burglars‚Äù for FREE - https://t.co/rsNJfyi8he üí© https://t.co/hLUBIBV2sD\",\n",
       " 'RABBLE! Rabble rabble rabble!',\n",
       " \"RT @tegridyfarms: Who's doing Dry January? https://t.co/mmh0HCFx7x\",\n",
       " 'Welcome to 2020! #SouthPark #NewYearsDay üçø https://t.co/Jk1xExOQQW',\n",
       " 'RT @tegridyfarms: Happy new year! ü§† #2020NewYear https://t.co/cVCqL6nWze',\n",
       " 'üéÜ Happy new year! üéá https://t.co/0TjzNwT6c0',\n",
       " 'Who are you kissing at midnight? üíã #NewYearsEve https://t.co/r1Es30f9x4',\n",
       " \"i'm sCoTt mAlKiNsOn i hAvE DiAbEtEs\",\n",
       " 'What games did you play this winter break? #southpark23 https://t.co/Xdwt9RdjgR',\n",
       " 'RT @tegridyfarms: Legalize it  ‚öñÔ∏è https://t.co/vBV0JdPrAG https://t.co/Nrzpoaagof',\n",
       " \"This @nprfreshair interview with Trey Parker and Matt Stone about @BookofMormon is one of NPR's staff picks of favorite interviews of the decade. Listen now üîî https://t.co/KsC07kk9KK\",\n",
       " 'RT @tegridyfarms: Farming with Tegridy. #tegridyfarms https://t.co/bm0ObPvXFH',\n",
       " \"Find out who's inside you. https://t.co/AUKw2RvfPP https://t.co/C2AhF2R0xl\",\n",
       " \"It's hard to be a Jew on Christmas. https://t.co/G1iazsN8qh https://t.co/27DfH1uOkX\",\n",
       " 'RT @tegridyfarms: #MerryChristmas from Tegridy Farms ‚ùÑÔ∏è https://t.co/MmAZ73btGQ',\n",
       " 'Howdy ho! #MerryChristmas from South Park https://t.co/52fncmyjlu',\n",
       " '@ormani_og @Mbata_c @Glamazontyomi https://t.co/TnVYBTP3Mv',\n",
       " 'What did Santa get you for Christmas? üéÅ#SouthPark https://t.co/oVJTGyEI5E',\n",
       " 'Happy birthday, Jesus üéÇ https://t.co/NkkHDONdMx',\n",
       " \"Where's Santa? üéÖüèª https://t.co/tKc7Vn8tMN\",\n",
       " 'Preparing for Christmas üéÑü¶åüêªüêø https://t.co/WrQG5OuCSo',\n",
       " \"What's your favorite song from Mr. Hankey's Christmas Classics album? üí© https://t.co/Mb8VFk3nWj\",\n",
       " 'Hark, hear the bells, sweet silver bells.\\nAll seem to say, \"Ding-dong, m\\'kay.\"\\nüîîüîîüîîüîî https://t.co/V5CwKSZaf1',\n",
       " \"Free Christmas episodes on https://t.co/2Zpt3EmI08 üéÖüèªüéÑüéÅ\\n\\nMr. Hankey the Christmas Poo\\nMerry Christmas Charlie Manson\\nMr. Hankey's Christmas Classics\\nA Very Crappy Christmas\\nRed Sleigh Down\\nIt's Christmas in Canada\\nWoodland Critter Christmas\\nThe Problem with a Poo\\nChristmas Snow\",\n",
       " \"What's on your wishlist this year? #southpark https://t.co/uxTGL888k8\",\n",
       " 'RED SLEIGH DOWN üõ∑and your other Christmas faves are playing today on @ComedyCentral https://t.co/zqSZtBdsXf',\n",
       " 'RT @I_B_Phil: @SouthPark https://t.co/rN4IMzjQL2',\n",
       " 'RT @Danny05841032: @SouthPark Just a bit of tegridy. https://t.co/OgGHaXvK6D',\n",
       " 'RT @Richtotheskies: @SouthPark https://t.co/SrFMrT6APF',\n",
       " 'RT @therealshyted: @SouthPark https://t.co/EIjeNkIenm',\n",
       " 'RT @graceofwrath: @SouthPark Oh Kenny tree, oh Kenny tree... üéÑ https://t.co/XdMij22kZ8',\n",
       " 'RT @JWackB: @SouthPark @Trey_Parker \\nTrying to finish painting the woodland critters but got to have barry the bear https://t.co/yxBJgv8gC2',\n",
       " 'RT @Narusbuns: @SouthPark Can‚Äôt have a happy holiday without these guys https://t.co/MMWkjlWO55',\n",
       " 'Have #SouthPark decoration on your tree or around the house? Tweet them to us! üéÑ https://t.co/YiKkubzX60',\n",
       " 'Wien-- wien-- wien-- winter. Winter is a cold time of year. #WinterSolstice https://t.co/jYmtiGzDvC',\n",
       " 'What are the first three words you see? https://t.co/tqDcgsjfhD',\n",
       " \"Can you name all of Santa's reindeer? https://t.co/GNhZsBqQEB\",\n",
       " 'To talk to Santa, call 719.838.4002 https://t.co/8ss21i5fas',\n",
       " 'Check out this brand new video from our friends @MontyPython: https://t.co/7DYIVOYLRs https://t.co/Wn96udIR8y',\n",
       " '@juuuullliiiaa https://t.co/Er9E1VH5br',\n",
       " 'Buckle up, buckaroo!',\n",
       " 'Christmas Snow ‚ùÑÔ∏è #SouthPark23 https://t.co/Tji3cr8Gen',\n",
       " 'Catch up on all of Season 23 of #SouthPark now on @Hulu! https://t.co/hnDqNG7RbB https://t.co/sce3Ojwap4',\n",
       " 'Happy birthday, Ike Broflovski! üá®üá¶ https://t.co/cZcg8S175I',\n",
       " 'What do you want for Christmas? #SouthPark23 https://t.co/tUrZLZe4nC',\n",
       " '@AquaMan72493 @tegridyfarms https://t.co/oEwrWR9Slz',\n",
       " '@PaulTheBookGuy 719.838.4002',\n",
       " 'Happy birthday, Sharon! You are so witty and alarmingly insightful üèñ https://t.co/Hsxi63oOvD',\n",
       " 'https://t.co/FZIfbKlBrT https://t.co/WIUm46SABo',\n",
       " 'üêøü¶åüêªü¶äü¶ùüê∞üê≠üå≤ https://t.co/4EgN3ZjAXi https://t.co/Qk4NDohOg4',\n",
       " '#SundayMorning üòá https://t.co/0ArsbU4c8Q',\n",
       " \"Nothin' like it. @tegridyfarms üèî https://t.co/ZAh2FPQvQP\",\n",
       " 'RT @tegridyfarms: Farm to Nostril. ‚ùÑÔ∏è https://t.co/algcRjqgQZ',\n",
       " 'ü§∑\\u200d‚ôÇÔ∏è \\nWatch the SEASON FINALE, ‚ÄúChristmas Snow‚Äù for FREE - https://t.co/xlTeebREsQ #SouthPark23 https://t.co/zn1EHyflcN',\n",
       " 'Friday the 13th üî™ https://t.co/kvdfiRlQhY',\n",
       " 'With the help of Santa and Jesus, Randy and @TegridyFarms return the Christmas Spirit to the people of #SouthPark. Watch the SEASON FINALE, ‚ÄúChristmas Snow‚Äù for FREE - https://t.co/xlTeebREsQ https://t.co/MewrwJVg8y',\n",
       " 'Do you like fish sticks? https://t.co/EGnuTZfRYi',\n",
       " '@Ghost_Nath Do you like fish sticks https://t.co/3tuiYp3BDh',\n",
       " 'Eek, A Penis! #SouthPark https://t.co/sndF22gC2J https://t.co/K2BeJSkVF2',\n",
       " 'RT @tegridyfarms: Coke Time üéÖüèª‚ùÑÔ∏è\\n\\nWatch now: https://t.co/vBV0JdPrAG https://t.co/ZgUv7g0UVX',\n",
       " 'When South Park bans marijuana, Randy and Towelie introduce the latest Tegridy product, bringing Christmas Spirits to a new high. Watch the SEASON FINALE, ‚ÄúChristmas Snow‚Äù for FREE - https://t.co/xlTeebREsQ #SouthPark23 https://t.co/7u2jVK85ue',\n",
       " '@heyyjonny ü§∑\\u200d‚ôÇÔ∏è',\n",
       " '‚ùÑÔ∏è‚ùÑÔ∏è‚ùÑÔ∏è #SouthPark23 ‚ùÑÔ∏è‚ùÑÔ∏è‚ùÑÔ∏è https://t.co/mYHAAgM8RK',\n",
       " '#SouthPark23 https://t.co/6vAufJ5QI0',\n",
       " '‚ùÑÔ∏è‚ùÑÔ∏è‚ùÑÔ∏è #SouthPark23 https://t.co/axFZHpPgU0',\n",
       " '#SouthPark23 https://t.co/Hu6Ixa8zSP',\n",
       " '#SouthPark23 https://t.co/zNyJXRs9Le',\n",
       " 'Hey East Coast. It‚Äôs a bleak Christmas Season in South Park and it‚Äôs all Santa‚Äôs fault. The town just wants their Christmas Spirit back but that will take a Christmas miracle. The #SouthPark23 #SeasonFinale starts NOW on @ComedyCentral https://t.co/lMSGDZyByy',\n",
       " 'Santa is stealing all the joy from the town‚Äôs Holiday Season in the #SouthPark23 #SeasonFinale ‚ÄúChristmas Snow‚Äù starting in ONE HOUR @ComedyCentral. https://t.co/KLnzdayV73',\n",
       " 'Santa is coming to #SouthPark in the Season 23 finale tonight! üéÖ https://t.co/vN9eVr6oYn',\n",
       " '@Sweet_Dck_Willy https://t.co/8qY5026Vzf',\n",
       " 'üéÖüèª Santa is stealing all the joy from the town‚Äôs Holiday Season in the Season 23 finale, ‚ÄúChristmas Snow,‚Äù airing tomorrow on Comedy Central. #SouthPark23 https://t.co/ynIhRkm21p',\n",
       " 'Views. üçë https://t.co/flSCe59zf1',\n",
       " 'Santa is stealing all the joy from the town‚Äôs Holiday Season in the Season 23 finale, ‚ÄúChristmas Snow,‚Äù airing on Wednesday, December 11, at 10:00p ET/PT on Comedy Central. #SouthPark23 https://t.co/0EYz5ZvvGX',\n",
       " \"Me: I'll be right there!\\nAlso me: https://t.co/AgIrZ3YvkO\",\n",
       " 'Are you a cop? #SouthPark23 https://t.co/K9ab0QSuOi',\n",
       " 'What is your favorite #SouthPark spin-off show?',\n",
       " \"What's your favorite food that you can't eat? #SouthPark23 https://t.co/aGGcISIcBr\"]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get user\n",
    "user = input('Enter user handle:')\n",
    "get_user = api.get_user(user)\n",
    "\n",
    "# get desired number of tweets\n",
    "count = input('How many tweets? (max 900)')\n",
    "\n",
    "tweets = api.user_timeline(screen_name=user, tweet_mode=\"extended\", count=count)\n",
    "tweets = [tweet.full_text for tweet in tweets]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movies</th>\n",
       "      <th>release</th>\n",
       "      <th>directors</th>\n",
       "      <th>star1</th>\n",
       "      <th>star2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Os Condenados de Shawshank</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>Tim Robbins</td>\n",
       "      <td>Morgan Freeman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>O Padrinho</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Marlon Brando</td>\n",
       "      <td>Al Pacino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>O Padrinho: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Al Pacino</td>\n",
       "      <td>Robert De Niro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>O Cavaleiro das Trevas</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Christian Bale</td>\n",
       "      <td>Heath Ledger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Doze Homens em F√∫ria</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>Henry Fonda</td>\n",
       "      <td>Lee J. Cobb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>Contra tempo</td>\n",
       "      <td>2016</td>\n",
       "      <td>Oriol Paulo</td>\n",
       "      <td>Mario Casas</td>\n",
       "      <td>Ana Wagener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>Aladdin</td>\n",
       "      <td>1992</td>\n",
       "      <td>Ron Clements</td>\n",
       "      <td>Scott Weinger</td>\n",
       "      <td>Robin Williams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>Guardi√µes da Gal√°xia</td>\n",
       "      <td>2014</td>\n",
       "      <td>James Gunn</td>\n",
       "      <td>Chris Pratt</td>\n",
       "      <td>Vin Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>A Can√ß√£o do Mar</td>\n",
       "      <td>2014</td>\n",
       "      <td>Tomm Moore</td>\n",
       "      <td>David Rawle</td>\n",
       "      <td>Brendan Gleeson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>O Of√≠cio de Matar</td>\n",
       "      <td>1967</td>\n",
       "      <td>Jean-Pierre Melville</td>\n",
       "      <td>Alain Delon</td>\n",
       "      <td>Fran√ßois P√©rier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         movies  release             directors  \\\n",
       "0    Os Condenados de Shawshank     1994        Frank Darabont   \n",
       "1                    O Padrinho     1972  Francis Ford Coppola   \n",
       "2          O Padrinho: Parte II     1974  Francis Ford Coppola   \n",
       "3        O Cavaleiro das Trevas     2008     Christopher Nolan   \n",
       "4          Doze Homens em F√∫ria     1957          Sidney Lumet   \n",
       "..                          ...      ...                   ...   \n",
       "245                Contra tempo     2016           Oriol Paulo   \n",
       "246                     Aladdin     1992          Ron Clements   \n",
       "247        Guardi√µes da Gal√°xia     2014            James Gunn   \n",
       "248             A Can√ß√£o do Mar     2014            Tomm Moore   \n",
       "249           O Of√≠cio de Matar     1967  Jean-Pierre Melville   \n",
       "\n",
       "              star1            star2  \n",
       "0       Tim Robbins   Morgan Freeman  \n",
       "1     Marlon Brando        Al Pacino  \n",
       "2         Al Pacino   Robert De Niro  \n",
       "3    Christian Bale     Heath Ledger  \n",
       "4       Henry Fonda      Lee J. Cobb  \n",
       "..              ...              ...  \n",
       "245     Mario Casas      Ana Wagener  \n",
       "246   Scott Weinger   Robin Williams  \n",
       "247     Chris Pratt       Vin Diesel  \n",
       "248     David Rawle  Brendan Gleeson  \n",
       "249     Alain Delon  Fran√ßois P√©rier  \n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the table data\n",
    "table_data = soup.find_all('td')\n",
    "\n",
    "# the names of the movies are under the image tag, specifically <img alt=\n",
    "movies = [t.find('img')['alt'] for t in table_data if t.find('img')]\n",
    "\n",
    "# people names are under title after href\n",
    "peoples = [t.a.get('title').replace(' (dir.)', '').split(', ') for t in table_data if (t.a and t.a.get('title'))]\n",
    "\n",
    "directors = [x[0] for x in peoples]\n",
    "star1 = [x[1] for x in peoples]\n",
    "star2 = [x[2] for x in peoples]\n",
    "year = [int(t.text[1:-1]) for t in soup.find_all('span', {'class':'secondaryInfo'})]\n",
    "\n",
    "\n",
    "imdb = pd.DataFrame(list(zip(movies, year, directors, star1, star2)), columns= ['movies', 'release', 'directors', 'star1', 'star2'])\n",
    "\n",
    "imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get links for every movie in the top250\n",
    "# the href appears multiple times under the a child so we need to set.\n",
    "links = list(set(['https://www.imdb.com' + t.a['href'] for t in table_data if t.a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sample of 10 movies from this list\n",
    "samples = random.sample(links, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title = []\n",
    "movie_year = []\n",
    "movie_summary = []\n",
    "\n",
    "# iterate over every link in the random sample\n",
    "for title in samples:\n",
    "    \n",
    "    # make the request\n",
    "    url = title\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # cook the soup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # extract title and year from h1 tag\n",
    "    movie_data = soup.find_all('h1')\n",
    "    mdata = [t.text.strip().split('\\xa0') for t in movie_data][0]\n",
    "    movie_title += [mdata[0]]\n",
    "    movie_year += [int(mdata[1].replace('(', '').replace(')', ''))]\n",
    "\n",
    "    # extract summary from div tag, specific class\n",
    "    summary = soup.find_all('div', {'class': 'summary_text'})\n",
    "    movie_summary += [[t.text.strip() for t in summary][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A Viagem de Chihiro</td>\n",
       "      <td>2001</td>\n",
       "      <td>During her family's move to the suburbs, a sul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Green Book - Um Guia Para a Vida</td>\n",
       "      <td>2018</td>\n",
       "      <td>A working-class Italian-American bouncer becom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Heat - Cidade Sob Press√£o</td>\n",
       "      <td>1995</td>\n",
       "      <td>A group of professional bank robbers start to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Incendies - A Mulher que Canta</td>\n",
       "      <td>2010</td>\n",
       "      <td>Twins journey to the Middle East to discover t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Antes do Amanhecer</td>\n",
       "      <td>1995</td>\n",
       "      <td>A young man and woman meet on a train in Europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Clube de Combate</td>\n",
       "      <td>1999</td>\n",
       "      <td>An insomniac office worker and a devil-may-car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Yojimbo - O Invenc√≠vel</td>\n",
       "      <td>1961</td>\n",
       "      <td>A crafty ronin comes to a town divided by two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Na Sombra e no Sil√™ncio</td>\n",
       "      <td>1962</td>\n",
       "      <td>Atticus Finch, a lawyer in the Depression-era ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Joker</td>\n",
       "      <td>2019</td>\n",
       "      <td>In Gotham City, mentally troubled comedian Art...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Matou</td>\n",
       "      <td>1931</td>\n",
       "      <td>When the police in a German city are unable to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  year  \\\n",
       "0               A Viagem de Chihiro  2001   \n",
       "1  Green Book - Um Guia Para a Vida  2018   \n",
       "2         Heat - Cidade Sob Press√£o  1995   \n",
       "3    Incendies - A Mulher que Canta  2010   \n",
       "4                Antes do Amanhecer  1995   \n",
       "5                  Clube de Combate  1999   \n",
       "6            Yojimbo - O Invenc√≠vel  1961   \n",
       "7           Na Sombra e no Sil√™ncio  1962   \n",
       "8                             Joker  2019   \n",
       "9                             Matou  1931   \n",
       "\n",
       "                                             summary  \n",
       "0  During her family's move to the suburbs, a sul...  \n",
       "1  A working-class Italian-American bouncer becom...  \n",
       "2  A group of professional bank robbers start to ...  \n",
       "3  Twins journey to the Middle East to discover t...  \n",
       "4  A young man and woman meet on a train in Europ...  \n",
       "5  An insomniac office worker and a devil-may-car...  \n",
       "6  A crafty ronin comes to a town divided by two ...  \n",
       "7  Atticus Finch, a lawyer in the Depression-era ...  \n",
       "8  In Gotham City, mentally troubled comedian Art...  \n",
       "9  When the police in a German city are unable to...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_movies = {'title': movie_title, 'year': movie_year, 'summary': movie_summary}\n",
    "random_movies = pd.DataFrame(random_movies)\n",
    "random_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>13:12:43</td>\n",
       "      <td>35.74N</td>\n",
       "      <td>117.54W</td>\n",
       "      <td>2.7</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>13:08:41</td>\n",
       "      <td>23.95S</td>\n",
       "      <td>67.37W</td>\n",
       "      <td>3.4</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>12:58:15</td>\n",
       "      <td>4.85N</td>\n",
       "      <td>96.07E</td>\n",
       "      <td>3.2</td>\n",
       "      <td>NORTHERN SUMATRA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>12:53:26</td>\n",
       "      <td>38.43N</td>\n",
       "      <td>39.11E</td>\n",
       "      <td>2.6</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>12:52:49</td>\n",
       "      <td>19.21N</td>\n",
       "      <td>155.46W</td>\n",
       "      <td>2.3</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>12:41:56</td>\n",
       "      <td>38.41N</td>\n",
       "      <td>39.03E</td>\n",
       "      <td>2.2</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>12:31:38</td>\n",
       "      <td>19.20N</td>\n",
       "      <td>155.47W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>11:56:00</td>\n",
       "      <td>35.71N</td>\n",
       "      <td>117.57W</td>\n",
       "      <td>2.5</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>11:48:41</td>\n",
       "      <td>38.37N</td>\n",
       "      <td>38.84E</td>\n",
       "      <td>2.0</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>11:45:56</td>\n",
       "      <td>2.84S</td>\n",
       "      <td>129.95E</td>\n",
       "      <td>3.1</td>\n",
       "      <td>SERAM, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>11:44:36</td>\n",
       "      <td>33.75S</td>\n",
       "      <td>70.95W</td>\n",
       "      <td>3.5</td>\n",
       "      <td>REGION METROPOLITANA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>11:32:13</td>\n",
       "      <td>19.18N</td>\n",
       "      <td>155.44W</td>\n",
       "      <td>2.3</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>11:04:56</td>\n",
       "      <td>36.81N</td>\n",
       "      <td>33.64E</td>\n",
       "      <td>2.9</td>\n",
       "      <td>CENTRAL TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>10:53:33</td>\n",
       "      <td>38.99N</td>\n",
       "      <td>27.87E</td>\n",
       "      <td>2.3</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>10:27:34</td>\n",
       "      <td>16.60S</td>\n",
       "      <td>179.80W</td>\n",
       "      <td>4.5</td>\n",
       "      <td>FIJI REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>10:13:28</td>\n",
       "      <td>17.79N</td>\n",
       "      <td>66.88W</td>\n",
       "      <td>3.2</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>09:42:25</td>\n",
       "      <td>4.64S</td>\n",
       "      <td>149.44E</td>\n",
       "      <td>5.4</td>\n",
       "      <td>BISMARCK SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>09:27:24</td>\n",
       "      <td>37.16N</td>\n",
       "      <td>28.69E</td>\n",
       "      <td>2.0</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>09:25:00</td>\n",
       "      <td>17.93N</td>\n",
       "      <td>66.68W</td>\n",
       "      <td>2.4</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>09:22:46</td>\n",
       "      <td>39.00N</td>\n",
       "      <td>27.87E</td>\n",
       "      <td>2.4</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>09:22:20</td>\n",
       "      <td>31.95S</td>\n",
       "      <td>70.26W</td>\n",
       "      <td>3.8</td>\n",
       "      <td>COQUIMBO, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>09:17:46</td>\n",
       "      <td>18.01N</td>\n",
       "      <td>66.87W</td>\n",
       "      <td>2.8</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>09:10:39</td>\n",
       "      <td>37.33N</td>\n",
       "      <td>26.72E</td>\n",
       "      <td>2.0</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>08:57:52</td>\n",
       "      <td>17.89N</td>\n",
       "      <td>66.86W</td>\n",
       "      <td>3.4</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>08:50:53</td>\n",
       "      <td>17.84N</td>\n",
       "      <td>66.87W</td>\n",
       "      <td>3.4</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>08:40:55</td>\n",
       "      <td>2.85S</td>\n",
       "      <td>129.96E</td>\n",
       "      <td>4.1</td>\n",
       "      <td>SERAM, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>08:37:05</td>\n",
       "      <td>35.53N</td>\n",
       "      <td>117.38W</td>\n",
       "      <td>2.2</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>08:35:37</td>\n",
       "      <td>17.92N</td>\n",
       "      <td>66.92W</td>\n",
       "      <td>2.6</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>08:29:05</td>\n",
       "      <td>13.16N</td>\n",
       "      <td>89.07W</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OFFSHORE EL SALVADOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>08:27:00</td>\n",
       "      <td>9.83S</td>\n",
       "      <td>78.79W</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NEAR COAST OF NORTHERN PERU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>08:23:14</td>\n",
       "      <td>17.90N</td>\n",
       "      <td>66.84W</td>\n",
       "      <td>3.1</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>08:19:21</td>\n",
       "      <td>38.40N</td>\n",
       "      <td>39.13E</td>\n",
       "      <td>2.3</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>08:00:29</td>\n",
       "      <td>34.05S</td>\n",
       "      <td>72.13W</td>\n",
       "      <td>3.1</td>\n",
       "      <td>OFFSHORE O'HIGGINS, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:53:47</td>\n",
       "      <td>34.07S</td>\n",
       "      <td>72.04W</td>\n",
       "      <td>3.0</td>\n",
       "      <td>OFFSHORE O'HIGGINS, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:52:46</td>\n",
       "      <td>16.16N</td>\n",
       "      <td>97.55W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OAXACA, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:46:24</td>\n",
       "      <td>43.12N</td>\n",
       "      <td>13.19E</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENTRAL ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:36:30</td>\n",
       "      <td>35.90N</td>\n",
       "      <td>140.50E</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NEAR EAST COAST OF HONSHU, JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:34:42</td>\n",
       "      <td>31.66N</td>\n",
       "      <td>104.36W</td>\n",
       "      <td>2.5</td>\n",
       "      <td>WESTERN TEXAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:27:58</td>\n",
       "      <td>17.91N</td>\n",
       "      <td>66.86W</td>\n",
       "      <td>3.2</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:25:05</td>\n",
       "      <td>39.39N</td>\n",
       "      <td>17.11E</td>\n",
       "      <td>2.2</td>\n",
       "      <td>SOUTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:24:18</td>\n",
       "      <td>64.07N</td>\n",
       "      <td>21.46W</td>\n",
       "      <td>3.3</td>\n",
       "      <td>ICELAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:21:45</td>\n",
       "      <td>38.10N</td>\n",
       "      <td>22.24E</td>\n",
       "      <td>2.7</td>\n",
       "      <td>GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:15:07</td>\n",
       "      <td>19.20N</td>\n",
       "      <td>155.39W</td>\n",
       "      <td>2.2</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:08:27</td>\n",
       "      <td>39.34N</td>\n",
       "      <td>17.15E</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SOUTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:05:40</td>\n",
       "      <td>52.84S</td>\n",
       "      <td>27.52E</td>\n",
       "      <td>5.1</td>\n",
       "      <td>SOUTH OF AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>07:02:13</td>\n",
       "      <td>39.03N</td>\n",
       "      <td>41.43E</td>\n",
       "      <td>3.2</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>06:44:05</td>\n",
       "      <td>19.20N</td>\n",
       "      <td>155.45W</td>\n",
       "      <td>2.1</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>06:43:28</td>\n",
       "      <td>17.72N</td>\n",
       "      <td>66.86W</td>\n",
       "      <td>2.5</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>06:39:39</td>\n",
       "      <td>16.46N</td>\n",
       "      <td>95.13W</td>\n",
       "      <td>4.2</td>\n",
       "      <td>OAXACA, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>06:23:05</td>\n",
       "      <td>19.22N</td>\n",
       "      <td>155.41W</td>\n",
       "      <td>2.5</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date      time latitude longitude magnitude  \\\n",
       "0   2020-02-09  13:12:43   35.74N   117.54W       2.7   \n",
       "1   2020-02-09  13:08:41   23.95S    67.37W       3.4   \n",
       "2   2020-02-09  12:58:15    4.85N    96.07E       3.2   \n",
       "3   2020-02-09  12:53:26   38.43N    39.11E       2.6   \n",
       "4   2020-02-09  12:52:49   19.21N   155.46W       2.3   \n",
       "5   2020-02-09  12:41:56   38.41N    39.03E       2.2   \n",
       "6   2020-02-09  12:31:38   19.20N   155.47W       2.0   \n",
       "7   2020-02-09  11:56:00   35.71N   117.57W       2.5   \n",
       "8   2020-02-09  11:48:41   38.37N    38.84E       2.0   \n",
       "9   2020-02-09  11:45:56    2.84S   129.95E       3.1   \n",
       "10  2020-02-09  11:44:36   33.75S    70.95W       3.5   \n",
       "11  2020-02-09  11:32:13   19.18N   155.44W       2.3   \n",
       "12  2020-02-09  11:04:56   36.81N    33.64E       2.9   \n",
       "13  2020-02-09  10:53:33   38.99N    27.87E       2.3   \n",
       "14  2020-02-09  10:27:34   16.60S   179.80W       4.5   \n",
       "15  2020-02-09  10:13:28   17.79N    66.88W       3.2   \n",
       "16  2020-02-09  09:42:25    4.64S   149.44E       5.4   \n",
       "17  2020-02-09  09:27:24   37.16N    28.69E       2.0   \n",
       "18  2020-02-09  09:25:00   17.93N    66.68W       2.4   \n",
       "19  2020-02-09  09:22:46   39.00N    27.87E       2.4   \n",
       "20  2020-02-09  09:22:20   31.95S    70.26W       3.8   \n",
       "21  2020-02-09  09:17:46   18.01N    66.87W       2.8   \n",
       "22  2020-02-09  09:10:39   37.33N    26.72E       2.0   \n",
       "23  2020-02-09  08:57:52   17.89N    66.86W       3.4   \n",
       "24  2020-02-09  08:50:53   17.84N    66.87W       3.4   \n",
       "25  2020-02-09  08:40:55    2.85S   129.96E       4.1   \n",
       "26  2020-02-09  08:37:05   35.53N   117.38W       2.2   \n",
       "27  2020-02-09  08:35:37   17.92N    66.92W       2.6   \n",
       "28  2020-02-09  08:29:05   13.16N    89.07W       5.0   \n",
       "29  2020-02-09  08:27:00    9.83S    78.79W       4.2   \n",
       "30  2020-02-09  08:23:14   17.90N    66.84W       3.1   \n",
       "31  2020-02-09  08:19:21   38.40N    39.13E       2.3   \n",
       "32  2020-02-09  08:00:29   34.05S    72.13W       3.1   \n",
       "33  2020-02-09  07:53:47   34.07S    72.04W       3.0   \n",
       "34  2020-02-09  07:52:46   16.16N    97.55W       4.0   \n",
       "35  2020-02-09  07:46:24   43.12N    13.19E       2.0   \n",
       "36  2020-02-09  07:36:30   35.90N   140.50E       4.0   \n",
       "37  2020-02-09  07:34:42   31.66N   104.36W       2.5   \n",
       "38  2020-02-09  07:27:58   17.91N    66.86W       3.2   \n",
       "39  2020-02-09  07:25:05   39.39N    17.11E       2.2   \n",
       "40  2020-02-09  07:24:18   64.07N    21.46W       3.3   \n",
       "41  2020-02-09  07:21:45   38.10N    22.24E       2.7   \n",
       "42  2020-02-09  07:15:07   19.20N   155.39W       2.2   \n",
       "43  2020-02-09  07:08:27   39.34N    17.15E       2.0   \n",
       "44  2020-02-09  07:05:40   52.84S    27.52E       5.1   \n",
       "45  2020-02-09  07:02:13   39.03N    41.43E       3.2   \n",
       "46  2020-02-09  06:44:05   19.20N   155.45W       2.1   \n",
       "47  2020-02-09  06:43:28   17.72N    66.86W       2.5   \n",
       "48  2020-02-09  06:39:39   16.46N    95.13W       4.2   \n",
       "49  2020-02-09  06:23:05   19.22N   155.41W       2.5   \n",
       "\n",
       "                            location  \n",
       "0                SOUTHERN CALIFORNIA  \n",
       "1                 ANTOFAGASTA, CHILE  \n",
       "2        NORTHERN SUMATRA, INDONESIA  \n",
       "3                     EASTERN TURKEY  \n",
       "4           ISLAND OF HAWAII, HAWAII  \n",
       "5                     EASTERN TURKEY  \n",
       "6           ISLAND OF HAWAII, HAWAII  \n",
       "7                SOUTHERN CALIFORNIA  \n",
       "8                     EASTERN TURKEY  \n",
       "9                   SERAM, INDONESIA  \n",
       "10       REGION METROPOLITANA, CHILE  \n",
       "11          ISLAND OF HAWAII, HAWAII  \n",
       "12                    CENTRAL TURKEY  \n",
       "13                    WESTERN TURKEY  \n",
       "14                       FIJI REGION  \n",
       "15                PUERTO RICO REGION  \n",
       "16                      BISMARCK SEA  \n",
       "17                    WESTERN TURKEY  \n",
       "18                PUERTO RICO REGION  \n",
       "19                    WESTERN TURKEY  \n",
       "20                   COQUIMBO, CHILE  \n",
       "21                       PUERTO RICO  \n",
       "22        DODECANESE ISLANDS, GREECE  \n",
       "23                PUERTO RICO REGION  \n",
       "24                PUERTO RICO REGION  \n",
       "25                  SERAM, INDONESIA  \n",
       "26               SOUTHERN CALIFORNIA  \n",
       "27                PUERTO RICO REGION  \n",
       "28              OFFSHORE EL SALVADOR  \n",
       "29       NEAR COAST OF NORTHERN PERU  \n",
       "30                PUERTO RICO REGION  \n",
       "31                    EASTERN TURKEY  \n",
       "32         OFFSHORE O'HIGGINS, CHILE  \n",
       "33         OFFSHORE O'HIGGINS, CHILE  \n",
       "34                    OAXACA, MEXICO  \n",
       "35                     CENTRAL ITALY  \n",
       "36  NEAR EAST COAST OF HONSHU, JAPAN  \n",
       "37                     WESTERN TEXAS  \n",
       "38                PUERTO RICO REGION  \n",
       "39                    SOUTHERN ITALY  \n",
       "40                           ICELAND  \n",
       "41                            GREECE  \n",
       "42          ISLAND OF HAWAII, HAWAII  \n",
       "43                    SOUTHERN ITALY  \n",
       "44                   SOUTH OF AFRICA  \n",
       "45                    EASTERN TURKEY  \n",
       "46          ISLAND OF HAWAII, HAWAII  \n",
       "47                PUERTO RICO REGION  \n",
       "48                    OAXACA, MEXICO  \n",
       "49          ISLAND OF HAWAII, HAWAII  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quake_date = [t.text.replace('earthquake', '').replace('\\xa0\\xa0\\xa0', '')[:10] for t in soup.find_all('td', {'class': 'tabev6'})]\n",
    "quake_time = [t.text.replace('earthquake', '').replace('\\xa0\\xa0\\xa0', '')[10:18] for t in soup.find_all('td', {'class': 'tabev6'})]\n",
    "quake_location = [t.text.replace('\\xa0', '') for t in soup.find_all('td', {'class': 'tb_region'})]\n",
    "\n",
    "# get latitude, longitude and cardinal directions\n",
    "quake_lat = [t.text.replace('\\xa0', '') for t in soup.find_all('td', {'class': 'tabev1'})][::2]\n",
    "quake_long = [t.text.replace('\\xa0', '') for t in soup.find_all('td', {'class': 'tabev1'})][1::2]\n",
    "quake_lat_cardinal = [t.text.replace('\\xa0', '') for t in soup.find_all('td', {'class': 'tabev2'})][::3]\n",
    "quake_long_cardinal = [t.text.replace('\\xa0', '') for t in soup.find_all('td', {'class': 'tabev2'})][1::3]\n",
    "\n",
    "# merge together both lat and long infos\n",
    "quake_latitude = [quake_lat[i] + quake_lat_cardinal[i] for i in range(len(quake_lat))]\n",
    "quake_longitude = [quake_long[i] + quake_long_cardinal[i] for i in range(len(quake_long))]\n",
    "\n",
    "# get magnitude\n",
    "quake_magnitude = [t.text.replace('\\xa0', '') for t in soup.find_all('td', {'class': 'tabev2'})][2::3]\n",
    "\n",
    "quakes = {'date': quake_date, \n",
    "          'time': quake_time, \n",
    "          'latitude': quake_latitude, \n",
    "          'longitude': quake_longitude, \n",
    "          'magnitude': quake_magnitude,\n",
    "          'location': quake_location}\n",
    "\n",
    "quakes = pd.DataFrame(quakes)\n",
    "\n",
    "quakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "#city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i didn't feel like creating an account but I might do this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# didn't quite understand what to do here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
